# refer to https://app.koofr.net/links/90cbd5aa-ef70-4f5e-99bc-f12e5a89380e?path=%2F2021-04-03T19-39-50_cin_transformer%2Fconfigs%2F2021-04-03T19-39-50-project.yaml
seed_everything: true
trainer:
  accelerator: gpu
  strategy: ddp_find_unused_parameters_true
  # strategy: 
  #   class_path: lightning.pytorch.strategies.FSDPStrategy
  #   init_args:
  #     sharding_strategy: "SHARD_GRAD_OP"
  devices: 8
  num_nodes: 6
  precision: 16-mixed
  max_epochs: 1000
  check_val_every_n_epoch: 12
  num_sanity_val_steps: -1
  gradient_clip_val: 1.0
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: "/path/to/your/save/ckpt/dir"
        save_top_k: -1
        monitor: "train/loss"
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: "/path/to/your/save/log/dir"
      version: "test"
      name:

model:
  class_path: src.WeTok.models.cond_transformer_gpt.Net2NetTransformer
  init_args:
    learning_rate: 3e-4
    first_stage_key: image
    cond_stage_key: class_label
    token_factorization: True
    weight_decay: 5e-2
    wpe: 0.1 ## learning rate decay
    wp: 6 ##no warmup
    wp0: 0.005 ##for warmup
    twde: 0
    transformer_config:
      target: src.WeTok.modules.transformer.gpt.GPT
      params:
        # vocab_size: 262144 # 262144 tokens
        vocab_size: 512
        block_size: 256
        spatial_n_layer: 24 ## follow LlamaGen
        factorized_n_layer: 2
        factorized_k: 4
        factorized_bits: [8, 8, 8, 8] #asymmetrical head
        n_head: 16
        dim: 1024
        cond_dim: 1024
        token_drop: 0.1
        resid_dropout_p: 0.1
        token_factorization: True
        class_num: 1000 #class tokens
    first_stage_config:
      target: src.WeTok.models.lfqgan.VQModel
      params:
        ckpt_path: /path/to/your/downsample16/channel32/tokenizer/ckpt
        n_embed: 256
        embed_dim: 32
        num_codebooks: 4
        learning_rate: 1e-4
        sample_minimization_weight: 1.0
        batch_maximization_weight: 1.0
        scheduler_type: "None"
        use_ema: False
        stage: "transformer"
        token_factorization: True
        factorized_bits: [8, 8, 8, 8]
        use_GFQ: True
        ddconfig:
          double_z: False
          z_channels: 32
          resolution: 128
          in_channels: 3
          out_ch: 3
          ch: 256
          ch_mult: [1,1,2,2,4]  # num_down = len(ch_mult)-1
          num_res_blocks: 4
        lossconfig:
          target: src.WeTok.modules.losses.DummyLoss
    cond_stage_config:
      target: src.WeTok.modules.util.Labelator
      params:
        n_classes: 1000
    permuter_config:
      target: src.WeTok.modules.transformer.permuter.ShiftPermuter
      params:
        shift_pos: 1000 # num_classes

data:
  class_path: main.DataModuleFromConfig
  init_args:
    batch_size: 64
    num_workers: 32
    train:
      target: src.WeTok.data.imagenet.ImageNetTrain
      params:
        config:
          size: 256
          subset:
    validation:
      target: src.WeTok.data.imagenet.ImageNetValidation
      params:
        config:
          size: 256
          subset:
    test:
      target: src.WeTok.data.imagenet.ImageNetValidation
      params:
        config:
          size: 256
          subset:

ckpt_path: null # to resume